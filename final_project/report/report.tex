\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Enemble Learning in Quora Question Intent Matching:
Using Parallel Naive Bayes, Word Embedding, and TF-IDF to Quantify Intent Differences}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Nathan M.~Brahmstadt \\
  Department of Electrical and Computer Engineering\\
  Oregon State University\\
  \texttt{brahmstn@oregonstate.edu} \\
  %% examples of more authors
  \And
  Jordan M.~Crane \\
  Department of Electrical and Computer Engineering \\
  Oregon State University \\
  \texttt{cranejo@oregonstate.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
    Our team set out to solve the problem set forth by the Kaggle Quora Question
    Pairs challenge; that is, to use machine learning and data mining techniques
    to identify Quora questions with similar intent. We aimed to build a model
    that given a pair of questions, could reliably identify whether the
    questions were duplicates or not. We pursued several avenues to find the
    best solution, but in the end we identified several 
    meaningful features from classifiers that analyzed shared and different words. Data was filtered 
    strategically to remove noise, fed into three classifiers, 
    and combined with a logistic regression model. Through this ensemble approach, we 
    achieved 0.4102 log-loss on our training data and 0.3552 log-loss on our testing data.

\end{abstract}

\section{Approaches explored}

In this section we detail the various approaches that we explored before
settling on our final solution.

\subsection{Preprocessing}

Our preprocessing is primarily carried out by our file parser, which takes the
raw CSV files and pares it down to something that is easier for our main
classifier to work with. It performs several modifications on the data while it
is being parsed.

\subsubsection{Abbreviation substitution}

One technique used by the parser is to look for common abbreviations and
substitute in the expanded version. This is an extremely useful technique, since
it provides the main classifier with more consistency across the data set.

\subsubsection{Stop words}

Another technique that we explored was the removal of stop words in the data.
This approach had both positive and negative ramifications: while it did reduce
the amount of fluff in the data, it also removed some intent-altering words,
such as when, where, and why. Ultimately, it resulted in lower log-loss so we
decided to include it in our final parser.

\subsubsection{Punctuation}

In our initial implementation, the function written to strip punctuation did not do
so properly, which was affecting our results significantly. However, when we
changed it to strip all punctuation, we actually saw an increase in our
log-loss. Further exploration revealed that this was caused by an interesting
feature of the question pairs: in many cases, the last word of the question holds
much greater weight with regard to the question's intent than the preceding
words. By not removing the punctuation, we were unintentionally setting the last
words apart since they were always considered by the classifier with the
question mark included. In order to maintain this unexpected benefit while still
taking adveantage of the gains provided by stripping punctuation, we decided to
remove all punctuation, but then duplicate the last word, including one copy
with the question mark and one without. This gave us the lowest log-loss of all
three configurations.

\subsubsection{Spell checking}

A large amount of error in our classifier is the result of one-off misspellings
in the dataset. To remedy this, we explored the idea of using a spell checker to
correct the errors in the data. Several factors steered us away from this
apporach. Firstly, we couldn't find a spell checking library that was fast
enough to avoid unacceptable slowdown in our parser. Secondly, correcting all
non-dictionary words to dictionary words can incur unintended side effects by
changing intent-altering words that do not appear in common dictionaries, such
as the names of foreign cities. Due to these considerations, we opted not to
implement spell checking in our final solution.

\subsection{Classifiers}

\subsubsection{Naive Bayes}

We chose this classifier right off the bat due to our familiarity with it and
how well it lends itself to the problem. However, previous implementations we
had seen were calssifying documents independently of one another, not comparing
two seperate texts as we are in this problem. In order to overcome this, we
built an ensemble with two Naive Bayes classifiers: one classifier looked at the
words which were common to the two questions, while the other looked at the set
of words which differed. We found that these two in conjunction were able to
make reasonable predictions on a large amount of the data, since most cases not
covered by one were caught by the other. If either classifier was certain that a
question was or was not a duplicate, it overrode the more uncertain classifier.

\subsubsection{Word embedding}

The other piece of our ensemble's top layer is Google's word2vec library
implementing a pre-trained Google News corpus of 3 million 300-dimensional
English word vectors. Word embedding is an interesting approach to natural
language processing which represents words as vectors in a high dimensional
space, such that words with similar usage in context will be placed similarly in
the space. To use this classifier, we took the average of the vector values
composing each question and then took the difference between these two values to
get an idea of how contextually similar the questions were. This was definitely
the least helpful of the classifiers used in our ensemble, but it provided a
significant reduction to our log-loss, and more insight into certain question
pairs. However, since we are using a pre-trained model word embedding falls
short when it comes to misspellings.

\subsubsection{Support vector machine}

The final piece of our ensemble is a support vector machine to tie all the other
components together. We chose to use an SVM for its simplicity and ability to
group the outputs of the previous classifiers with wide margin.

\subsubsection{Convolutional neural network}

We also explored using a convolutional neural network to take the place of the
SVM, but found it to be overly complex for the task. Additionally, we worried
that a more complex model would overfit, and were unsure of our ability to
combat this in an ensemble.

\subsubsection{Long short-term memory network}

\subsubsection{Logistic Regression}

\subsection{Features}

\subsubsection{Common words}

\subsubsection{Unique words}

\subsubsection{Total frequency-inverse document frequency}

\subsubsection{Word sentiment}

\section{Results}

In this section we explore the final solution upon which we settled. We based the structure of our algorithm with the following idea in mind: generate as many meaningful features as possible from the data, and use them to train a logistic regression model. 

\subsubsection{Preprocessing}

Before the data was input into classifiers, there were many decisions considered 


\subsubsection{Common Word and Differing Word Naive Bayes Models}

\subsubsection{Sentence Similarity using Word Embedding}

\subsubsection{Term Frequency - Inverse Document Frequency}

\subsubsection{Logistic Regression}


\section{Conclusion}
\label{headings}

Our goal was to create meaningful classifiers exploring the concept of "differing and shared words" as identifiers of intent between two questions. The resulting ensemble approach unified three meaningful classifiers that we had discovered, and obtained a 0.35526 log-loss on the test data. This score is by no means ground-breaking, but we are satisfied by the result anyways. Our approach was able to perform above average in the competition, and we feel that the success it had, based on its relatively simple algorithm, exceeded our expectations.

\subsection{Headings: second level}

Second-level headings should be in 10-point type.

\subsubsection{Headings: third level}

Third-level headings should be in 10-point type.

\paragraph{Paragraphs}

There is also a \verb+\paragraph+ command available, which sets the
heading in bold, flush left, and inline with the text, with the
heading followed by 1\,em of space.

\section{Citations, figures, tables, references}
\label{others}

These instructions apply to everyone.

\subsection{Citations within the text}

The \verb+natbib+ package will be loaded for you by default.
Citations may be author/year or numeric, as long as you maintain
internal consistency.  As to the format of the references themselves,
any style is acceptable as long as it is used consistently.

The documentation for \verb+natbib+ may be found at
\begin{center}
  \url{http://mirrors.ctan.org/macros/latex/contrib/natbib/natnotes.pdf}
\end{center}
Of note is the command \verb+\citet+, which produces citations
appropriate for use in inline text.  For example,
\begin{verbatim}
   \citet{hasselmo} investigated\dots
\end{verbatim}
produces
\begin{quote}
  Hasselmo, et al.\ (1995) investigated\dots
\end{quote}

If you wish to load the \verb+natbib+ package with options, you may
add the following before loading the \verb+nips_2017+ package:
\begin{verbatim}
   \PassOptionsToPackage{options}{natbib}
\end{verbatim}

If \verb+natbib+ clashes with another package you load, you can add
the optional argument \verb+nonatbib+ when loading the style file:
\begin{verbatim}
   \usepackage[nonatbib]{nips_2017}
\end{verbatim}

As submission is double blind, refer to your own published work in the
third person. That is, use ``In the previous work of Jones et
al.\ [4],'' not ``In our previous work [4].'' If you cite your other
papers that are not widely available (e.g., a journal paper under
review), use anonymous author names in the citation, e.g., an author
of the form ``A.\ Anonymous.''

\subsection{Footnotes}

Footnotes should be used sparingly.  If you do require a footnote,
indicate footnotes with a number\footnote{Sample of the first
  footnote.} in the text. Place the footnotes at the bottom of the
page on which they appear.  Precede the footnote with a horizontal
rule of 2~inches (12~picas).

Note that footnotes are properly typeset \emph{after} punctuation
marks.\footnote{As in this example.}

\subsection{Figures}

All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction. The figure number and caption
always appear after the figure. Place one line space before the figure
caption and one line space after the figure. The figure caption should
be lower case (except for first word and proper nouns); figures are
numbered consecutively.

You may use color figures.  However, it is best for the figure
captions and the paper body to be legible if the paper is printed in
either black/white or in color.
\begin{figure}[h]
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}

\subsection{Tables}

All tables must be centered, neat, clean and legible.  The table
number and title always appear before the table.  See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.

Note that publication-quality tables \emph{do not contain vertical
  rules.} We strongly suggest the use of the \verb+booktabs+ package,
which allows for typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.

\begin{table}[t]
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule{1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Final instructions}

Do not change any aspects of the formatting parameters in the style
files.  In particular, do not modify the width or length of the
rectangle the text should fit into, and do not change font sizes
(except perhaps in the \textbf{References} section; see below). Please
note that pages should be numbered.

\section{Preparing PDF files}

Please prepare submission files with paper size ``US Letter,'' and
not, for example, ``A4.''

Fonts were the main cause of problems in the past years. Your PDF file
must only contain Type 1 or Embedded TrueType fonts. Here are a few
instructions to achieve this.

\begin{itemize}

\item You should directly generate PDF files using \verb+pdflatex+.

\item You can check which fonts a PDF files uses.  In Acrobat Reader,
  select the menu Files$>$Document Properties$>$Fonts and select Show
  All Fonts. You can also use the program \verb+pdffonts+ which comes
  with \verb+xpdf+ and is available out-of-the-box on most Linux
  machines.

\item The IEEE has recommendations for generating PDF files whose
  fonts are also acceptable for NIPS. Please see
  \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

\item \verb+xfig+ "patterned" shapes are implemented with bitmap
  fonts.  Use "solid" shapes instead.

\item The \verb+\bbold+ package almost always uses bitmap fonts.  You
  should use the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or
\verb+\mathbb{C}+ for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You
can also use the following workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the
\verb+amssymb+ package.

\end{itemize}

If your file contains type 3 fonts or non embedded TrueType fonts, we
will ask you to fix it.

\subsection{Margins in \LaTeX{}}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always
specify the figure width as a multiple of the line width as in the
example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})

A number of width problems arise when \LaTeX{} cannot properly
hyphenate a line. Please give LaTeX hyphenation hints using the
\verb+\-+ command when necessary.

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include
acknowledgments in the anonymized submission, only in the final paper.

\section*{Appendix: Contribution Levels}

\subsection*{Nathan}

\subsection*{Jordan}

\end{document}
